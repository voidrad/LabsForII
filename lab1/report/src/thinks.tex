\section{Выводы}
В ходе выполнения лабораторной работы я познакомился с линейными моделями классического машинного обучения: логистической регрессией, 
методом опорных векторов, наивным байесовским классификатором и методом k-ближайших соседей. Больше всего заинтересовал алгоритм 
k-ближайших соседей для классификации, так как он кажется интуитивно лучшим, ведь позволяет разделять классы не прямой, как в случае SVM,
а проводить границу, фактически, по точкам. Однако его недостаток в большой вычислительной сложности и в том, что если нет большой корреляции 
между признаком и результатом, то алгоритм банально будет часто ошибаться.

Основная сложность в работе --- реализация каждого метода вручную, пришлось искать очень много методов из библиотек numpy и scikit-learn, 
чтобы легко работать с данными. Пожалуй, это заняло большую часть времени.

В результате набор данных удалось разделить максимум с точностью 85\%. Я связываю это с тем, что некоторые признаки слабо коррелировали с целевой
переменной, а также с тем, что количества данных для обучения банально слишком мало. Конечно, в медицинских задача метрика точности
не должна являться ключевым показателем, но во время проведения ЛР я сравнивал результаты работы разных алгоритмов по ней.
\pagebreak
